"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.updatePineconeIndex = exports.createPineconeIndexIfNotExist = exports.queryPineconeVectorStoreAndQueryLLM = void 0;
const openai_1 = require("langchain/embeddings/openai");
const text_splitter_1 = require("langchain/text_splitter");
const openai_2 = require("langchain/llms/openai");
const chains_1 = require("langchain/chains");
const document_1 = require("langchain/document");
// Function to query the Pinecone vector store and Query Language Model
const queryPineconeVectorStoreAndQueryLLM = (client, indexName, question) => __awaiter(void 0, void 0, void 0, function* () {
    // Starting the query process
    console.log("Querying Pinecone vector store...");
    // Retrieve the specific Pinecone index
    const index = client.Index(indexName);
    // Create an embedding of the question to query
    const queryEmbedding = yield new openai_1.OpenAIEmbeddings().embedQuery(question);
    // Query the Pinecone index and return top 10 matches
    let queryResponse = yield index.query({
        queryRequest: {
            topK: 10,
            vector: queryEmbedding,
            includeMetadata: true,
            includeValues: true,
        },
    });
    // Log the question being asked
    console.log(`Asking question: ${question}...`);
    if (queryResponse && queryResponse.matches && queryResponse.matches.length) {
        // Log the number of matches found
        console.log(`Found ${queryResponse.matches.length} matches...`);
        // Create an OpenAI instance and load the QAStuffChain
        const instructions = "You're an AI assistant. Based on the following excerpts from a long document, provide a conversational answer to the question asked. If the answer isn't in the context, simply respond with 'Hmm, I'm not sure.' Don't invent an answer. If the question isn't related to the context, state that you are programmed to answer questions relevant to the given context. Remember, you cannot use images or visual content to form your answer. Do the answer in less than 2000 caracters.";
        const preparedQuestion = `${instructions}\n\n${question}`;
        const llm = new openai_2.OpenAI({ modelName: "gpt-3.5-turbo-16k" });
        const chain = (0, chains_1.loadQAStuffChain)(llm);
        // Extract the page content from the matched documents and concatenate
        const concatenatedPageContent = queryResponse.matches
            .map((match) => match.metadata.pageContent)
            .join(" ");
        const documentLinks = queryResponse.matches.map((match) => match.metadata.docLink);
        // Execute the chain with input documents and question
        const result = yield chain.call({
            input_documents: [new document_1.Document({ pageContent: concatenatedPageContent })],
            question: preparedQuestion,
        });
        // Log the answer generated by GPT-3
        console.log(`Answer: ${result.text}`);
        // Return the answer and the links to the source documents
        return {
            answer: result.text,
            documentLinks: documentLinks,
        };
    }
    else {
        // If no matches were found, inform the user that GPT-3 won't be queried
        console.log("Since there are no matches, GPT-3.5 will not be queried.");
    }
});
exports.queryPineconeVectorStoreAndQueryLLM = queryPineconeVectorStoreAndQueryLLM;
const createPineconeIndexIfNotExist = (client, indexName, vectorDimension, timeout = 180000) => __awaiter(void 0, void 0, void 0, function* () {
    // Check if the index exists
    console.log(`Checking "${indexName}"...`);
    const existingIndexes = yield client.listIndexes();
    // If index does not exist, create it
    if (!existingIndexes.includes(indexName)) {
        // 4. Log index creation initiation
        console.log(`Creating "${indexName}"...`);
        // 5. Create index
        yield client.createIndex({
            createRequest: {
                name: indexName,
                dimension: vectorDimension,
                metric: "cosine",
            },
        });
        // 6. Log successful creation
        console.log(`Creating index.... please wait for it to finish initializing.`);
        // Wait for XXX seconds to let the index initialize
        yield new Promise((resolve) => setTimeout(resolve, timeout));
    }
    else {
        // If the index exists, log that it already exists
        console.log(`"${indexName}" already exists.`);
    }
});
exports.createPineconeIndexIfNotExist = createPineconeIndexIfNotExist;
// Function to update Pinecone Index with new vectors
const updatePineconeIndex = (client, indexName, docs) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("Retrieving Pinecone index...");
    // Retrieve the specific Pinecone index
    const index = client.Index(indexName);
    console.log(`Pinecone index retrieved: ${indexName}`);
    // Process each document in the docs array
    for (const doc of docs) {
        console.log(`Processing document: ${doc.metadata.source}`);
        const txtPath = doc.metadata.source;
        const text = doc.pageContent;
        const documentLinkMatch = text.match(/Link: (.+)/);
        const documentLink = documentLinkMatch ? documentLinkMatch[1] : "";
        // Create an instance of RecursiveCharacterTextSplitter
        const textSplitter = new text_splitter_1.RecursiveCharacterTextSplitter({
            chunkSize: 1000,
        });
        console.log("Splitting text into chunks...");
        // Split text into chunks (documents)
        const chunks = yield textSplitter.createDocuments([text]);
        console.log(`Text split into ${chunks.length} chunks`);
        console.log(`Calling OpenAI's Embedding endpoint documents with ${chunks.length} text chunks ...`);
        // Create embeddings for the documents
        const embeddingsArrays = yield new openai_1.OpenAIEmbeddings().embedDocuments(chunks.map((chunk) => chunk.pageContent.replace(/\n/g, " ")));
        console.log("Finished embedding documents");
        console.log(`Creating ${chunks.length} vectors array with id, values, and metadata...`);
        // Create and upsert vectors in batches of 100
        const batchSize = 100;
        let batch = [];
        for (let idx = 0; idx < chunks.length; idx++) {
            const chunk = chunks[idx];
            const vector = {
                id: `${txtPath}_${idx}`,
                values: embeddingsArrays[idx],
                metadata: Object.assign(Object.assign({}, chunk.metadata), { loc: JSON.stringify(chunk.metadata.loc), pageContent: chunk.pageContent, txtPath: txtPath, docLink: documentLink }),
            };
            batch = [...batch, vector];
            // When batch is full or it's the last item, upsert the vectors
            if (batch.length === batchSize || idx === chunks.length - 1) {
                yield index.upsert({
                    upsertRequest: {
                        vectors: batch,
                    },
                });
                // Empty the batch
                batch = [];
            }
        }
        // Log the number of vectors updated
        console.log(`Pinecone index updated with ${chunks.length} vectors`);
    }
});
exports.updatePineconeIndex = updatePineconeIndex;
